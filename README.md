# Emoji2Idiom 
ðŸ¤— ðŸ”¥ ðŸ”¥ The novel benchmark for MLLMs focuses on emoji understanding! ðŸ”¥

## Introduction of Emoji2Idiom
ðŸ¤— ðŸ”¥ ðŸ”¥ ðŸ”¥ Vision and Language are two major modalities in Artificial Intelligence research.
How to bridge the gap between vision and language has always been the goal of researchers in the multimodal community.
Inspired by human behavior, we believe that if a model can see an image and directly associate it with its linguistic meaning, the model possesses high-level intelligence that spans vision and language.
In our work, we focus on emojis in images, which we regard as a data form with both visual and linguistic characteristics. 
Specifically, we first propose the task of translating Emojis in images to corresponding idioms, thereby challenging the ability of Multimodal Large Language Models (MLLMs) to (1) understand the semantic correlation between language and emojis, and (2) reason the intricate linguistic meaning from the emojis in images.
To facilitate the advancement of this task, we construct a high-quality benchmark (\ourdata{}) following the process of automatic model generation and human manual filtering. 
Based on our constructed \ourdata{}, we employ multiple advanced MLLMs to conduct extensive experiments and detailed analyses, demonstrating that existing MLLMs do not yet have enough capability to understand and reason the linguistic information from visual data.
We believe our proposed benchmark and interesting discoveries will encourage the community to attach importance to the intelligence of MLLMs directly associating language from vision, to give MLLMs more comprehensive vision-language understanding ability.

![introductionimage](/images/Introduction.png "Illustration of Emoji2Idiom")
